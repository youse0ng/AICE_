{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWV0y8NsK32xEu+bp4sCzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youse0ng/AICE_/blob/main/AICE_09_%EB%B9%84%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5%EC%9C%BC%EB%A1%9C_AI%EB%AA%A8%EB%8D%B8%EB%A7%81%ED%95%98%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 차원 축소\n",
        "\n",
        "비지도 학습은 입력데이터에 대한 목표값 없이 학습시키는 머신러닝 방법이다.\n",
        "\n",
        "레이블링이 되어 있지 않은 데이터에서 패턴이나 특성을 찾아야 하기 때문에, 지도학습보단 난이도가 있다.\n",
        "\n",
        "비지도학습은 파악하기 어려운 문제를 찾아내거나 인식하지 못한 데이터의 특징을 알아낼때 도움이 되며, 주로 연관 있는 것들을 찾고 그룹핑하는 군집화 방식을 사용한다.\n",
        "\n",
        "지도학습에서 적절한 특성(Feature)을 찾아내기 위한 전처리 방법으로 비지도학습을 사용하기도 한다.\n",
        "\n",
        "머신러닝은 알고리즘을 사용하여 데이터에서 패턴을 찾는데, 학습데이터에 특성의 수가 적으면 머신러닝의 모델 성능이 떨어지고, 특성이 또 너무 많으면 학습 데이터에 과대적합될 가능성이 있다.\n",
        "\n",
        "머신러닝에서 차원 축소는 데이터의 차원을 변환하므로 일부 정보 손실이 발생할 수 있다.\n",
        "\n",
        "그러므로 원본 데이터의 정보 손실을 최소화하면서 원본 데이터를 저차원으로 다시 표현하는 것이 관건이며 적재적소에 활용하는 것이 중요하다.\n",
        "\n"
      ],
      "metadata": {
        "id": "NilrxTlgj50E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "차원 축소 방법에는 두가지 방법이 존재한다.\n",
        "\n",
        "  1. 특성 선택\n",
        "  2. 특성 추출\n",
        "\n",
        "특성 선택은 훈련에 가장 유용한 특성을 선택하는 것으로, 모델의 정확도를 향상하기 위해 원본 데이터에서 가장 좋은 성능을 보여줄 수 있는 데이터의 부분집합을 찾아내는 방법이다.\n",
        "\n",
        "특성 추출은 기존 특성을 반영해서 저차원의 중요 특성으로 압축하는 것으로, 주어진 데이터를 더 잘 설명할 수 있는 새로운 특성을 추출한다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_t4rcr5lXmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 주성분 분석"
      ],
      "metadata": {
        "id": "FE-SCgy9l8ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 주성분 분석 이해\n",
        "\n",
        "주성분 분석 (PCA)는 데이터의 분산을 최대한 유지하면서 특성이 많은 데이터세트의 차원을 줄이는 방법이다.\n",
        "\n",
        "주성분 분석 알고리즘은 원본 데이터의 분산을 최대한 보존하는 새로운 축을 찾고, 그 축에 데이터를 투영해서 차원을 축소한다.\n",
        "\n",
        "즉, 기존의 특성을 조합하여 고차원 공간의 데이터들을 저차원 공간으로 변환하는 새로운 특성 세트를 찾습니다.\n",
        "\n",
        "새로운 특성을 주성분이라고 하며, 이들은 서로 직교(독립적)하며 원본 데이터를 나타낼 수 있다.\n",
        "\n",
        "일반적으로 주성분 분석 결과에서 누적 기여율이 80~90퍼를 차지하는 주성분들로 개수를 선택합니다.\n",
        "\n",
        "첫 번째 주성분(PC1)이 원본 데이터의 특성을 가장 많이 보존하고, 두 번째 주성분 (PC2)가 원본 데이터의 특성을 그다음으로 많이 보존한다.\n",
        "\n",
        "예를 들어, 원본 데이터가 20차원인 경우 기존의 특성들을 조합하여 주성분을 만들 수 있으며, PC1,PC2,PC3가 원본 데이터 정보의 90%를 보존한다면, 분석에 큰 무리가 없으므로, PC1,PC2,PC3만 선택하여 3차원 데이터로 차원을 줄일 수 있다.\n",
        "\n",
        "고차원 공간의 데이터들을 저차원 공간으로 변환하여 데이터의 차원을 축소하면 시각화와 계산이 용이하여 쉽게 분석할 수 있다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M24yetasmEwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 주성분 분석 사용 목적\n",
        "\n",
        "주성분 분석의 사용 목적은 첫 번째, 시각화입니다.\n",
        "\n",
        "데이터가 3차원 이상이 되면, 사람 눈으로 인지하기 어려워지는데,\n",
        "\n",
        "차원 축소를 하면 시각화를 통해 데이터의 패턴을 파악할 수 있다.\n",
        "\n",
        "두 번째 목적은 노이즈 제거입니다. 쓸모없는 특성을 없애서 노이즈를 줄일 수 있다.\n",
        "\n",
        "마지막으로 주성분 분석은 복잡성을 줄이기 위한 데이터 전처리로 많이 활용\n",
        "\n",
        "1.   시각화\n",
        "2.   노이즈제거\n",
        "3.   데이터 전처리\n",
        "\n"
      ],
      "metadata": {
        "id": "3Ih3OYs1mFMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 주성분 개수 설정 기준\n",
        "\n",
        "주성분 개수를 설정하는 기준은 주로 고윳값과 누적 기여율입니다.\n",
        "\n",
        "주성분은 데이터를 정규화한 후,\n",
        "\n",
        "사이킷런 라이브러리 내 decomposition 서브패키지의 PCA클래스로 간단하게 구할 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "HFM2W2k7mFKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca=PCA(n_composition='사용하고자 하는 주성분의 개수')\n",
        "pc=pca.fit_transform(df_scaled)"
      ],
      "metadata": {
        "id": "HXeu5MPvmFI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'PCA를 완료하면 기여율이 높은 순서대로 주성분들이 정렬됩니다. 기여율은 하나의 주성분이 원본 데이터들 얼만큼 잘 반영하는가 나타내는 값이고,\n",
        "\n",
        "0~1 사이긔 값으로 표현된다.\n",
        "\n",
        "고차원 데이터는 기여율 합(누적 기여율)이 80~90%를 넘는 정도까지 활용합니다.\n",
        "\n",
        "기여율은 `explained_variance_ratio_` 속성으로 확인가능.\n",
        "\n"
      ],
      "metadata": {
        "id": "ixe0NrVhmFGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratio=pca.explained_variance_ratio_\n",
        "print(ratio)"
      ],
      "metadata": {
        "id": "w62ZVSphmFE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 주성분 분석 실습하기"
      ],
      "metadata": {
        "id": "5PaVMOlImFDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 합성데이터 생성하기 make_blobs"
      ],
      "metadata": {
        "id": "AZvKCy6omFA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 불러오기 (make_blobs 데이터셋)\n",
        "from sklearn.datasets import make_blobs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 실습용 데이터세트 생성하기\n",
        "x,y=make_blobs(n_features=10,\n",
        "               n_samples=1000,\n",
        "               centers=5,\n",
        "               random_state=2023,\n",
        "               cluster_std=1)\n",
        "plt.scatter(x[:,0],x[:,1],c=y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8HKfjxGomE9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터세트 표준화하기\n",
        "\n",
        "데이터세트의 모든 특성의 중요도를 동일하게 취급하기 위해 PCA를 적용하기 전 데이터세트에 표준화를 적용합니다.\n",
        "\n",
        "사이킷런 preprocessing 서브패키지의 StandardScaler 클래스로 데이터세트를 표준화합니다.\n",
        "\n",
        "StandardScaler는 특성들의 평균을 0, 분산 1로 스케일링한다.\n",
        "특성들을 정규분포로 변환\n",
        "\n"
      ],
      "metadata": {
        "id": "36A491QwmE26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 불러오기\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 데이터 표준화하기\n",
        "scaler=StandardScaler()\n",
        "scaler.fit(x)\n",
        "std_data=scaler.transform(x)\n",
        "\n",
        "print(x)\n",
        "print()\n",
        "print(std_data)"
      ],
      "metadata": {
        "id": "jgmaFoKK3sWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 주성분 분석 수행하기\n",
        "\n",
        "주성분 분석 기능은 사이킷런 라이브러리의 decomposition 서브패키지에서 PCA 클래스로 제공됩니다.\n",
        "\n",
        "적절한 주성분 개수를 설정하기 위해 n_components를 원본 데이터의 feature 개수 10로 설정해서 PCA를 수행합니다.\n"
      ],
      "metadata": {
        "id": "GCbPwJ983sSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 불러오기\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PCA 객체로 주성분 10개 추출하기\n",
        "pca=PCA(n_components=10)\n",
        "reduced_data=pca.fit_transform(std_data)\n",
        "\n",
        "# 주성분 데이터 확인\n",
        "pca_df=pd.DataFrame(reduced_data)\n",
        "pca_df.head()"
      ],
      "metadata": {
        "id": "6Tg_-T993sQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA 알고리즘으로 주성분을 추출하면 설명된 분산(Explained Variance)값이 높은 순서대로 주성분들이 정렬된다.\n",
        "\n",
        "`explained_variance_` 속성을 통해 주성분의 분석 설명력을 확인할 수 있으며, 값이 클수록 좋다"
      ],
      "metadata": {
        "id": "qqxO6-ge3sOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 설명된 분산 값 확인하기\n",
        "print(pca.explained_variance_)\n"
      ],
      "metadata": {
        "id": "ALs4bbFw3sM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "`explained_variance_ratio_` 속성에 저장된 설명된 분산 비율은 각 주성분의 축을 따라 놓여있는 데이터세트의 분산 비율을 나타내는 유용한 정보입니다.\n",
        "\n",
        "값을 보면, 데이터세트 분산의 45%가 첫번째 PC1, 30%가 두 번째 PC2를 따라 놓여있다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XLf3vzRt7J3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 설명된 분산 비율 확인하기\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "qShIARgx3sLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "주성분의 고윳값(explained_variance), 기여율(explained_variance_ratio), 누적 기여율(cumulative explained variances)를 같이 출력해봅니다.\n",
        "\n",
        "기여율을 계산하는 공식\n",
        "\n",
        "기여율 = 특정 주성분의 분산에 대한 비율 = 특정 주성분 분산 / 모든 주성분 분산의 합"
      ],
      "metadata": {
        "id": "FrWX4rew3sJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_data.shape"
      ],
      "metadata": {
        "id": "SF1DRkwd8YlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 불러오기\n",
        "import numpy as np\n",
        "\n",
        "# 주성분의 설명력과 기여율 구하기\n",
        "index=np.array([f'pca{n+1}' for n in range(reduced_data.shape[1])])\n",
        "result=pd.DataFrame({'고윳값':pca.explained_variance_,\n",
        "                     '기여율':pca.explained_variance_ratio_},\n",
        "                    index=index)\n",
        "\n",
        "result['누적기여율']=result['기여율'].cumsum()\n",
        "\n",
        "# 주성분의 설명력과 기여율 확인하기\n",
        "display(result)"
      ],
      "metadata": {
        "id": "xVl20G4y3sFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "일반적으로 고윳값이 0.7 이상 또는 1인 주성분을 기준으로 하여 누적 기여율이 0.8~ 0.9 이상일 때 적절한 주성분 개수를 설정할 수 있다.\n",
        "\n",
        "위 결과를 보면 pca1, pca2, pca3, pca4의 고윳값이 0.7이상이고, pca1,pca2,pca3,pca4의 누적기여율이 약 96.5%이므로,\n",
        "\n",
        "차원 축소를 위한 가장 적절한 주성분 개수는 4개로 판단할 수 있다.\n",
        "\n",
        "데이터 시각화를 위해서는 일반적으로 2차원 또는 3차원으로 축소합니다.\n",
        "\n",
        "n_components=4로 설정하여 PCA를 다시 실행한다."
      ],
      "metadata": {
        "id": "SQg_N55w3sDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA 객체로 주성분 4개 추출하기\n",
        "pca=PCA(n_components=4)\n",
        "\n",
        "X_reduced=pca.fit_transform(std_data)\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "-2sYiLv33sAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사이킷런에서 제공하는 PCA클래스를 사용할 때, 주성분의 개수를 지정하기보단 보존하려는 분산 비율을 n_components에 0.0~1.0의 값으로 설정하는 것이 편리합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "glIhGAJHmE1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 지정한 비율에 도달할 때까지 주성분을 탐색하기\n",
        "pca=PCA(n_components=0.9)\n",
        "reduced_data=pca.fit_transform(std_data)\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "vsJ9NhNZ_Yh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-분산 확률적 이웃 임베딩"
      ],
      "metadata": {
        "id": "A0gTaenY_oPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE 이해하기\n",
        "\n",
        "t-분산 확률적 이웃 임베딩은 높은 차원의 복잡한 데이터를 2차원 또는 3차원으로 축소하는 방법입니다.\n",
        "\n",
        "PCA는 선형변환으로 차원 축소를 하는 방법이고, t-SNE는 비선형적인 방법의 차원 축소입니다.\n",
        "\n",
        "t-SNE는 복잡한 데이터의 시각화에 주로 사용하며 차원 축소할 때는 비슷한 데이터들로 정리된 상태이므로 데이터 구조를 이해하는데 도움이 된다.\n",
        "\n",
        "t-SNE 알고리즘은 고차원 공간에서 데이터들의 유사성과 그에 해당하는 저차원 공간에서 데이터들의 유사성을 계산합니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "daz4-mA-_tuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### t-SNE 실습하기"
      ],
      "metadata": {
        "id": "B29DAUaK_tsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 합성 데이터 생성하기 make_blobs"
      ],
      "metadata": {
        "id": "1CCm9nZK_tqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# 실습용 데이터세트 생성하기\n",
        "x,y=make_blobs(\n",
        "    n_features=10,\n",
        "    n_samples=100,\n",
        "    centers=3,\n",
        "    random_state=42,\n",
        "    cluster_std=2\n",
        ")\n",
        "plt.scatter(x[:,0],x[:,1],c=y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "do2h6QKn_toq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2차원 t-SNE 시각화하기\n",
        "\n",
        "사이킷런 라이브러리 manifold 서브패키지에서 제공하는 TSNE클래스로 임베딩을 생성합니다.\n",
        "\n",
        "n_components는 차원의 개수를 결정하는 인자로, n_components=2로 설정하여 2차원으로 임베딩을 만들어 봅니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4_D6AXkf_tmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# 2차원 t-SNE 임베딩하기\n",
        "tsne_np=TSNE(n_components=2,random_state=1).fit_transform(x)\n",
        "\n",
        "# Numpy array를 DataFrame으로 변환하기\n",
        "tsne_df=pd.DataFrame(tsne_np,columns=['component 1','component 2'])\n",
        "print(tsne_df)"
      ],
      "metadata": {
        "id": "k8xhufZn_tka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-SNE는 비선형적으로 차원을 축소하기 때문에 표현력이 증가하고, 다음의 시각화 결과와 같이 t-SNE로 차원 축소한 경우 클래스 간에 분별력있게 시각화하는 장점이있다."
      ],
      "metadata": {
        "id": "exyce0qy_tep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# class target 정보 불러오기\n",
        "tsne_df['target']=y\n",
        "\n",
        "# target 별 분류하기\n",
        "tsne_df0=tsne_df[tsne_df['target']==0]\n",
        "tsne_df1=tsne_df[tsne_df['target']==1]\n",
        "tsne_df2=tsne_df[tsne_df['target']==2]\n",
        "\n",
        "# target별 시각화하기\n",
        "plt.scatter(tsne_df0['component 1'],tsne_df0['component 2'],\n",
        "            color='pink',label=\"A\")\n",
        "plt.scatter(tsne_df1['component 1'],tsne_df1['component 2'],\n",
        "            color='purple',label='B')\n",
        "plt.scatter(tsne_df2['component 1'],tsne_df1['component 2'],\n",
        "            color='green',label='C')\n",
        "\n",
        "plt.xlabel(\"Component 1\")\n",
        "plt.ylabel(\"Component 2\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UZCwuj7n_tbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3차원으로 t-SNE 시각화하기\n",
        "\n",
        "TSNE 객체(n_components=3)로 설정하여 3차원 임베딩을 만들어보자"
      ],
      "metadata": {
        "id": "cbjGYRqBFTFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3차원 t-SNE 임베딩하기\n",
        "tsne_np=TSNE(n_components=3,\n",
        "             random_state=15).fit_transform(x)\n",
        "\n",
        "# Numpy array를 DataFrame으로 변환\n",
        "tsne_df=pd.DataFrame(tsne_np,columns=['Component 0', 'Component 1','Component 2'])\n",
        "\n",
        "print(tsne_df)"
      ],
      "metadata": {
        "id": "BkS_CIzZ_tZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 3차원 그래프 세팅하기\n",
        "fig=plt.figure(figsize=(10,10))\n",
        "ax=fig.add_subplot(111,projection='3d')\n",
        "\n",
        "# class target 정보 불러오기\n",
        "tsne_df['target']=y\n",
        "\n",
        "# target별 분리하기\n",
        "tsne_df_0=tsne_df[tsne_df['target']==0]\n",
        "tsne_df_1=tsne_df[tsne_df['target']==1]\n",
        "tsne_df_2=tsne_df[tsne_df['target']==2]\n",
        "\n",
        "# target 별 시각화하기\n",
        "ax.scatter(tsne_df_0['Component 0'],\n",
        "           tsne_df_0['Component 1'],\n",
        "           tsne_df_0['Component 2'],\n",
        "           color='pink',label='A'\n",
        "           )\n",
        "ax.scatter(tsne_df_1['Component 0'],\n",
        "           tsne_df_1['Component 1'],\n",
        "           tsne_df_1['Component 2'],\n",
        "           color='red',label='B'\n",
        "           )\n",
        "ax.scatter(tsne_df_2['Component 0'],\n",
        "           tsne_df_2['Component 1'],\n",
        "           tsne_df_2['Component 2'],\n",
        "           color='yellow',label='B'\n",
        "           )\n",
        "\n",
        "ax.set_xlabel('Component 0')\n",
        "ax.set_ylabel('Component 1')\n",
        "ax.set_zlabel('Component 2')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vdXGLod7_tHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 군집화"
      ],
      "metadata": {
        "id": "HsWQl_yTPIrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-평균 군집화\n"
      ],
      "metadata": {
        "id": "LCxN8dd5PKZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-평균 군집화 이해하기\n",
        "군집화는 비슷한 요소들을 묶는것을 의미한다.\n",
        "\n",
        "일반적으로 시각화 가능한 차원의 데이터 (2차원 또는 3차원)이 제공되면, 사람의 눈은 매우 쉽게 별개의 군집을 구분할 수 있다.\n",
        "\n",
        "그러나 기계가 한 번에 군집을 구분하는 것은 쉽지 않기 때문에, 군집화 알고리즘이 등장했다.\n",
        "\n",
        "군집화 알고리즘은 인간의 눈으로 볼 수 없는 고차원의 데이터도 군집화 할 수 있다.\n",
        "그것이 바로 K-평균 군집화 알고리즘이다.\n",
        "\n",
        "K-평균 군집화 알고리즘은 K개의 군집을 나누기 위해 각 군집의 중심과 데이터 간의 평균 거리를 활용한다.\n",
        "\n",
        "따라서 군집 내 거리 계산의 기준이 될 점을 K개로 설정하고, 해당 점에서 다른 데이터간의 거리를 계산한다.\n",
        "\n",
        "이때 최종 군집의 기준점은 보통 클러스터의 중심에 위치하기 떄문에, 센트로이드(Centroid, 클러스터 중심점)이라고 부른다.\n",
        "\n",
        "K-평균 군집화는 데이터 간 거리가 가까운 데이터를 비슷한 특징을 가진 데이터로 간주하여 기준점과 다른 데이터 간의 거리가 최소화되는 점을 찾아다니면서 군집화한다.\n",
        "\n",
        "즉, 각 군집 내 분산을 최소화하는 것을 목적으로 군집화를 진행한다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ynV1DUZPNni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-평균 군집화 알고리즘 작동 방식 이해하기"
      ],
      "metadata": {
        "id": "6S19lm7xQ5ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. 군집화할 군집의 수 K를 선택한다.\n",
        "\n",
        "2. 임의로 K개의 기준점을 선택한다.\n",
        "  - 임의로 K개의 기준점을 선택해 군집을 찾는 프로세스를 시작. 이 포인트들은 우리가 만들 군집의 중심 역할을 한다.\n",
        "\n",
        "3. K개의 군집을 만듭니다.\n",
        "  - 군집을 만들기 위해 먼저 데이터 포인트에서 3개의 중심까지 거리를 측정하여 가장 가까운 군집에 포인트를 할당합니다.\n",
        "\n",
        "4. 각 군집의 새 중심을 계산\n",
        "  - 3개의 군집이 있으므로 각각에 의해 형성된 새로운 중심을 찾는다.\n",
        "\n",
        "5. 각 군집의 품질 평가\n",
        "  - 모든 군집 내의 분산을 찾아 군집화의 품질 측정, 군집 내 분산이 최소화되도록 군집을 정의하는 것\n",
        "    WCSS를 계산한다.\n",
        "\n",
        "6. 3~5단계를 반복"
      ],
      "metadata": {
        "id": "h0-lInbwQ-o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-평균 군집화 실습\n",
        "\n",
        "sklearn.cluster import KMeans\n",
        "\n",
        "하이퍼 파라미터\n",
        "- n_clusters: 클러스터의 개수\n",
        "- max_iter: 알고리즘의 최대 반복횟수\n",
        "\n",
        "군집화가 끝나면 객체가 갖는 속성\n",
        "- labels_: 군집 번호\n",
        "- cluster_centers: 군집별 기준점의 좌표\n",
        "\n"
      ],
      "metadata": {
        "id": "y6aWOLhvTmAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import Kmeans\n",
        "import numpy as np\n",
        "\n",
        "# 임의의 데이터 생성\n",
        "X=np.array([[1,2],[1,4],[1,0],[10,2],[10,4],[10,0]])\n",
        "\n",
        "# K-평균 군집화 알고리즘 모델생성\n",
        "kmeans=KMeans(n_clusters=2,random_state=0).fit(X)\n",
        "\n",
        "# K-평균 군집화 알고리즘 결과\n",
        "print(f\"kmeans의 라벨링 결과 : {kmeans.labels_}\")\n",
        "print(f\"kmeans.predict [[0,0],[12,3]] = {kmeans.predict([[0,0],[12,3]])}\")\n",
        "print(f\"kmeans.cluster_centers_ (군집별 기준점의 좌표 Centroid : {kmeans.cluster_centers_}\")"
      ],
      "metadata": {
        "id": "HFowS1VLTo55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DBSCAN"
      ],
      "metadata": {
        "id": "mYcRMN4N9cOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN 이해하기\n",
        "\n",
        "데이터가 밀집한 정도를 기반으로 군집화하는 알고리즘\n",
        "\n",
        "어떤 점을 기준으로 반경 (E: 엡실론)내에 점이 n개 이상있으면 하나의 군집으로 인식하는 방식으로, 점이 몰려 있어서 밀도가 높은 부분을 클러스터링하는 방식입니다.\n",
        "\n",
        "DBSCAN은 단순히 가까운 거리의 데이터를 군집화하는 것이아니라, 밀도 높게 모여있는 데이터들을 군집화할 때 유용합니다.\n",
        "\n",
        "가까이 연결되어 밀도가 높은 데이터들을 같은 군집으로 분류한다.\n",
        "\n",
        "\n",
        "DBSCAN의 장점은 K-Means와 같이 군집의 수를 정하지 않아도 되며,\n",
        "\n",
        "군집의 밀도에 따라 서로 연결하기 때문에, 기하학적인 모양을 갖는 군집도 잘 찾을 수 있고, 노이즈 포인트를 통해 이상치 검출이 가능하다는 것이다.\n",
        "\n",
        "\n",
        "1. 기하학적으로 분포되어 있는 군집도 잘 분류\n",
        "2. 노이즈 포인트로 이상치 탐지를 잘 파악할 수 있음\n",
        "\n"
      ],
      "metadata": {
        "id": "mBaW75619mr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN 알고리즘 작동 원리\n",
        "\n",
        "1. 데이터 p가 존재할 때, E(엡실론) 최소 거리를 기준으로 엡실론 반경 내에 몇 개의 데이터가 위치하는지를 센다. 이때, 엡실론 반경 내에 위치한 데이터가 min_points의 개수를 넘는 경우 p의 이웃이 되고 같은 군집으로 분류된다.\n",
        "\n",
        "2. 핵심 데이터(Core Point)는 군집 내의 밀집된 지역에 있는 것으로, 해당 점을 기준으로 엡실론 반경 내에 MinPts**(밀집 지역을 정의하기 위한 필요한 이웃의 개수)**개 이상의 데이터가 있는 경우 핵심데이터가 된다.\n",
        "핵심 데이터의 엡실론 반경 내의 데이터는 모두 동일한 클러스터에 속함\n",
        "\n",
        "3. 경계 데이터(Border point)는 군집 내에 속하지만 해당 점을 기준으로 엡실론 반경 내에 MinPts개 미만의 데이터가 있는 것이다.\n",
        "\n",
        "4. 핵심 데이터 또는 경계 데이터도 아닌 데이터는 노이즈(Noise Point)로 간주 = 이상치\n",
        "\n"
      ],
      "metadata": {
        "id": "EkHXUnuqABav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN 실습\n",
        "sklearn.clust import DBSCAN\n",
        "\n",
        "DBSCAN의 파라미터\n",
        "1. eps : 이웃을 정의하기 위한 거리\n",
        "\n",
        "2. min_samples: 군집을 정의하기 위한 eps 반경 내 최소 데이터 갯수\n",
        "\n",
        "\n",
        "DBSCAN 객체 속성\n",
        "\n",
        "1. labels_: 군집 번호(노이즈의 경우  -1)\n",
        "\n",
        "2. `core_sample_indices_`: 핵심데이터의 인덱스\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rw2tm8wrB9W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "# 실습용 데이터 생성\n",
        "centers=[[1,1],[-1,-1],[1,-1]]\n",
        "X, labels_true=make_blobs(\n",
        "    n_samples=750,\n",
        "    centers=centers,\n",
        "    cluster_std=0.4,\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "# 데이터 표준화하기\n",
        "# fit_transform은 fit()과 transform()을 합한 메소드임\n",
        "X=StandardScaler().fit_transform(X)\n",
        "\n",
        "print(X.shape)\n",
        "# 데이터 시각화\n",
        "plt.scatter(X[:,0],X[:,1])\n",
        "plt.show"
      ],
      "metadata": {
        "id": "5ltnnYCCCItt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn import metrics\n",
        "\n",
        "# DBSCAN 모델 정의 및 학습\n",
        "db=DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
        "labels=db.labels_\n",
        "print(labels)\n",
        "\n",
        "# Noisy samples를 제외한 클러스터 개수 확인\n",
        "n_clusters_ =len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "# Noisy samples 개수 확인하기\n",
        "n_noise_=list(labels).count(-1)\n",
        "\n",
        "print(\"Estimated number of Clusters : %d\" % n_clusters_)\n",
        "print(\"Estimated number of noise Points: %d \" % n_noise_)"
      ],
      "metadata": {
        "id": "rd8rO-gbFz-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 평가지표 출력하기\n",
        "print(f'Homogeneity(동질성): {metrics.homogeneity_score(labels_true,labels):.3f}')\n",
        "print(f\"Completeness(완전성): {metrics.completeness_score(labels_true,labels):.3f}\")\n",
        "print(f\"V-measure: {metrics.v_measure_score(labels_true,labels):.3f}\")\n",
        "print(f\"Rand_index(랜드지수): {metrics.rand_score(labels_true,labels):.3f}\")\n",
        "print(f\"AMI:조정된 랜드 지수 {metrics.adjusted_rand_score(labels_true,labels):.3f}\")\n",
        "print(f\"Silhouette Coefficient: {metrics.silhouette_score(X,labels):.3f}\")\n",
        "# 실루엣 계수는 실제 레이블을 알 수 없는 경우 모델결과 자체를 사용해야만 평가를 수행할 수있다.\n"
      ],
      "metadata": {
        "id": "6gRrJMX4LBdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 시각화하기\n"
      ],
      "metadata": {
        "id": "6M5MRNMJNg6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "핵심 데이터 core_points 와 경계 데이터(board_point)는 각각 큰 점과 작은 점으로 시각화되며, 할당된 클러스터에 따라 색상으로 구분된다.\n",
        "\n",
        "noise_point 태그가 지정된 샘플은 검은색으로 표시된다.\n"
      ],
      "metadata": {
        "id": "iVmI88FILBZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 유일값 설정\n",
        "unique_labels=set(labels) # 중복 제거\n",
        "\n",
        "core_samples_mask=np.zeros_like(labels,dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_]=True"
      ],
      "metadata": {
        "id": "NnJk8E12Ns34"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}